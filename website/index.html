<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/png" href="images/favicon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header>
            <h1>Deep Ignorance</h1>
            <p class="subtitle">Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</p>
            <div class="authors">
                <div class="author-row">
                    <span class="author">Kyle O'Brien<sup>1*</sup></span>
                    <span class="author">Stephen Casper<sup>2*</sup></span>
                </div>
                <div class="author-row">
                    <span class="author">Quentin Anthony<sup>1</sup></span>
                    <span class="author">Tomek Korbak<sup>2</sup></span>
                    <span class="author">Robert Kirk<sup>2</sup></span>
                    <span class="author">Xander Davies<sup>2,3</sup></span>
                    <span class="author">Ishan Mishra<sup>2</sup></span>
                </div>
                <div class="author-row">
                    <span class="author">Geoffrey Irving<sup>2</sup></span>
                    <span class="author">Yarin Gal<sup>2,3</sup></span>
                    <span class="author">Stella Biderman<sup>1</sup></span>
                </div>
            </div>
            <div class="affiliations">
                <div><sup>1</sup>EleutherAI &nbsp;&nbsp; <sup>2</sup>UK AISI &nbsp;&nbsp; <sup>3</sup>University of Oxford</div>
                <div class="equal-contrib"><sup>*</sup>Equal Contribution</div>
                <div class="emails">kyle@eleuther.ai &nbsp;&nbsp; scasper@mit.edu</div>
            </div>
        </header>

        <div class="figure">
            <img src="images/results.png" alt="Results" class="responsive-img">
            <p class="caption"><strong>Figure 1: Training data filtering makes LLMs resistant to adversarial fine-tuning without sacrificing general performance.</strong> Models whose training data has been filtered to remove text related to dual-use biology topics (left) have unaffected general capabilities and (right) have low biothreat proxy capabilities and resist up to 10,000 steps and 300M tokens of adversarial fine-tuning.</p>
        </div>

        <div class="quote-block">
            <blockquote>
                <p>"Open weights allow global research communities to both advance capabilities and address model flaws by providing them with direct access to a critical AI component that is prohibitively expensive for most actors to develop independently. However, the open release of model weights could also pose risks of facilitating malicious or misguided use or perpetuating model flaws and biases. Once model weights are available for public download, there is no way to implement a wholesale rollback of all existing copies of the model."</p>
                <cite>â€” <a href="https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf" target="_blank">International AI Safety Report</a> (Bengio et al., 2025)</cite>
            </blockquote>
        </div>

        <nav>
            <a href="#" class="nav-link external" target="_blank">
                Paper
            </a>
            <a href="https://huggingface.co/collections/EleutherAI/deep-ignorance-685441040d024a0fee593d68" class="nav-link external" target="_blank">
                <svg width="20" height="20" viewBox="0 0 32 32" fill="currentColor">
                    <path d="M16.1 6.9c-3.9 0-7.1 3.2-7.1 7.1 0 1.3.4 2.5 1 3.6l-4.5 4.5c-.6.6-.6 1.5 0 2.1.3.3.7.4 1 .4s.7-.1 1-.4l4.5-4.5c1.1.6 2.3 1 3.6 1 3.9 0 7.1-3.2 7.1-7.1s-3.2-7.1-7.1-7.1zm0 11.3c-2.3 0-4.2-1.9-4.2-4.2s1.9-4.2 4.2-4.2 4.2 1.9 4.2 4.2-1.9 4.2-4.2 4.2z"/>
                    <circle cx="16" cy="14" r="2.5"/>
                </svg>
                Models+Data
            </a>
            <a href="https://github.com/EleutherAI/deep-ignorance" class="nav-link external" target="_blank">
                <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M12 2C6.477 2 2 6.477 2 12c0 4.42 2.865 8.17 6.839 9.49.5.092.682-.217.682-.482 0-.237-.008-.866-.013-1.7-2.782.603-3.369-1.34-3.369-1.34-.454-1.156-1.11-1.462-1.11-1.462-.908-.62.069-.608.069-.608 1.003.07 1.531 1.03 1.531 1.03.892 1.529 2.341 1.087 2.91.831.092-.646.35-1.086.636-1.336-2.22-.253-4.555-1.11-4.555-4.943 0-1.091.39-1.984 1.029-2.683-.103-.253-.446-1.27.098-2.647 0 0 .84-.269 2.75 1.025A9.578 9.578 0 0 1 12 6.836a9.59 9.59 0 0 1 2.504.337c1.909-1.294 2.747-1.025 2.747-1.025.546 1.377.203 2.394.1 2.647.64.699 1.028 1.592 1.028 2.683 0 3.842-2.339 4.687-4.566 4.935.359.309.678.919.678 1.852 0 1.336-.012 2.415-.012 2.743 0 .267.18.578.688.48C19.138 20.167 22 16.418 22 12c0-5.523-4.477-10-10-10z"/>
                </svg>
                Code
            </a>
        </nav>

        <main>
            <section id="introduction">
                <h2>Introduction</h2>
                
                <p>
                    A risk as LLMs grow more capable is that they learn unsafe knowledge during training. These dangerous capabilities could be exploited by bad actors. Today's safeguards' primary focus is on suppressing unsafe knowledge in post-training, often via refusal training.
                </p>
                <p>
                    Open-weight models, those that users can download and modify locally, offer unique benefits related to transparency, research, and the deconcentration of power. However, they are vulnerable to "tampering attacks" that can remove their safety training. For instance, it is straightforward to train open-weight models never to refuse unsafe requests. This is of increasing concern as open-weight models begin to rival the capabilities of the best closed-weight models.
                </p>
                <p>
                    We explore an intuitive yet understudied question: Can we prevent LLMs from learning unsafe technical capabilities (such as CBRN) by filtering out enough of the relevant pretraining data before we begin training a model? We train multiple 6.9B LLMs from scratch on an unfiltered dataset and on filtered versions where we filtered out biorisk knowledge. We observe three main results:
                </p>
                <ol>
                    <li><strong>Knowledge Prevention:</strong> The filtered models perform significantly worse on our biorisk knowledge evaluations, nearly at random chance. Crucially, filtering does not lead to notable regressions in general knowledge. These results suggest that data filtering may be a simple way to prevent models from learning dangerous capabilities without sacrificing utility.</li>
                    <li><strong>Tamper-Resistance:</strong> Open-weight models can be fine-tuned by downstream users on biorisk data. We study this attack by fine-tuning our models on 300M tokens of high-quality biorisk-related documents. We find that performance can improve, but that it is still well below the no-filtering baseline. Data filtering is significantly more tamper-resistant than current safeguards.</li>
                    <li><strong>Defense-in-Depth:</strong> We demonstrate that data filtering cannot prevent LLMs from leveraging harmful knowledge provided in-context, but that Circuit-Breaking-based techniques offer complementary defenses. However, we show that none of the defenses we test are resistant to staged attacks that combine fine-tuning and in-context retrieval.</li>
                </ol>
                <p>
                    Taken together, these results suggest that rigorous pretraining data filtering is a promising method for preventing acquisition of dangerous technical capabilities without obvious degradation in overall model utility. Our efficient data approach allowed us to perform filtering with less than a 1% increase in training compute (FLOPS). We release our models, optimizer states, and intermediate checkpoints to enable future research into domains including data-driven AI security, pretraining research, machine unlearning, and mechanistic interpretability. We are especially excited to see future work that addresses our limitations, such as studying the filtering of other types of knowledge and developing scaling trends. See our paper for more details, discussion, and sketches of future work.
                </p>
                
                <div class="figure">
                    <img src="images/filtering_method.png" alt="Filtering Method" class="responsive-img">
                    <p class="caption"><strong>Figure 2: Our multi-stage data filtering pipeline: Our goal is to filter out data related to unwanted topics.</strong> We study biothreat-proxy knowledge as a representative example. All documents undergo initial "blocklist" filtering, where those without prohibited terms are retained without further review. Documents containing blocked terms (e.g., "pathogen(s)") are escalated to a fine-tuned text classifier that evaluates semantic content. The classifier assigns probability scores for unsafe content: documents scoring below the predetermined threshold are retained, while those exceeding it are excluded from the training corpus. In practice, the vast majority of documents are approved by the blocklist and thus do not require review by the classifier stage.</p>
                </div>
            </section>

            <section id="artifacts">
                <h2>Released Artifacts</h2>
                <p>
                    All models and datasets are available on our <a href="https://huggingface.co/collections/EleutherAI/deep-ignorance-685441040d024a0fee593d68" target="_blank">HuggingFace collection</a>.
                </p>
                
                <div class="table-container">
                    <table class="artifacts-table">
                        <thead>
                            <tr>
                                <th>Model Name</th>
                                <th>Description</th>
                                <th>Filtering Strategy</th>
                                <th>Defense Type</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="section-header">
                                <td colspan="4"><strong>Baseline Models</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-unfiltered">deep-ignorance-unfiltered</a></td>
                                <td>Baseline model without filtering</td>
                                <td>None</td>
                                <td>None</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-pretraining-stage-unfiltered">deep-ignorance-pretraining-stage-unfiltered</a></td>
                                <td>Pretraining checkpoint (500B tokens)</td>
                                <td>None</td>
                                <td>None</td>
                            </tr>
                            
                            <tr class="section-header">
                                <td colspan="4"><strong>Core Filtered Models</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-strong-filter">deep-ignorance-e2e-strong-filter</a></td>
                                <td>End-to-end strong filtering</td>
                                <td>Blocklist only (8.42% removed)</td>
                                <td>Data filtering</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-weak-filter">deep-ignorance-e2e-weak-filter</a></td>
                                <td>End-to-end weak filtering</td>
                                <td>Blocklist + ModernBERT</td>
                                <td>Data filtering</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-extra-weak-filter">deep-ignorance-e2e-extra-weak-filter</a></td>
                                <td>End-to-end extra weak filtering</td>
                                <td>Blocklist + ModernBERT (relaxed)</td>
                                <td>Data filtering</td>
                            </tr>
                            
                            <tr class="section-header">
                                <td colspan="4"><strong>Hybrid Filtering Models</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-strong-filter-pt-weak-filter-anneal">deep-ignorance-strong-filter-pt-weak-filter-anneal</a></td>
                                <td>Hybrid approach</td>
                                <td>Strong pretraining, weak annealing</td>
                                <td>Data filtering</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-weak-filter-pt-strong-filter-anneal">deep-ignorance-weak-filter-pt-strong-filter-anneal</a></td>
                                <td>Reverse hybrid approach</td>
                                <td>Weak pretraining, strong annealing</td>
                                <td>Data filtering</td>
                            </tr>
                            
                            <tr class="section-header">
                                <td colspan="4"><strong>Circuit Breaking (CB) Variants</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-unfiltered-cb">deep-ignorance-unfiltered-cb</a></td>
                                <td>Baseline + Circuit Breaking</td>
                                <td>None</td>
                                <td>CB post-training</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-strong-filter-cb">deep-ignorance-e2e-strong-filter-cb</a></td>
                                <td>Strong filter + Circuit Breaking</td>
                                <td>Blocklist only</td>
                                <td>Data filtering + CB</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-strong-filter-pt-weak-filter-anneal-cb">deep-ignorance-strong-filter-pt-weak-filter-anneal-cb</a></td>
                                <td>Hybrid + Circuit Breaking</td>
                                <td>Strong PT, weak anneal</td>
                                <td>Data filtering + CB</td>
                            </tr>
                            
                            <tr class="section-header">
                                <td colspan="4"><strong>CB + Latent Adversarial Training (LAT) Variants</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-unfiltered-cb-lat">deep-ignorance-unfiltered-cb-lat</a></td>
                                <td>Baseline + CB + LAT</td>
                                <td>None</td>
                                <td>CB + LAT post-training</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-strong-filter-cb-lat">deep-ignorance-e2e-strong-filter-cb-lat</a></td>
                                <td>Strong filter + CB + LAT</td>
                                <td>Blocklist only</td>
                                <td>Maximum defense</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-strong-filter-pt-weak-filter-anneal-cb-lat">deep-ignorance-strong-filter-pt-weak-filter-anneal-cb-lat</a></td>
                                <td>Hybrid + CB + LAT</td>
                                <td>Strong PT, weak anneal</td>
                                <td>Maximum defense</td>
                            </tr>
                            
                            <tr class="section-header">
                                <td colspan="4"><strong>Knowledge Corruption Variants</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-strong-filter-weak-knowledge-corrupted">deep-ignorance-e2e-strong-filter-weak-knowledge-corrupted</a></td>
                                <td>Strong filter + weak corruption</td>
                                <td>Blocklist + synthetic corruption</td>
                                <td>Data filtering + corruption</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-strong-filter-strong-knowledge-corrupted">deep-ignorance-e2e-strong-filter-strong-knowledge-corrupted</a></td>
                                <td>Strong filter + strong corruption</td>
                                <td>Blocklist + radical corruption</td>
                                <td>Data filtering + corruption</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
            </section>

            <section id="citation">
                <h2>Citation</h2>
                <p>If you use this work in your research, please cite:</p>
                
                <div class="citation-box">
                    <pre><code>@article{obrien2025deep,
  title={Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs},
  author={O'Brien, Kyle and Casper, Stephen and Gurnee, Wes and Rajaram, Achyuta and others},
  journal={arXiv preprint arXiv:2025.xxxxx},
  year={2025}
}</code></pre>
                </div>
            </section>
        </main>

        <footer>
            <p>Contact: <a href="https://www.kyobrien.io/" target="_blank">Kyle O'Brien</a> | <a href="https://stephencasper.com/" target="_blank">Stephen Casper</a></p>
        </footer>
    </div>
</body>
</html>